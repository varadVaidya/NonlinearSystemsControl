\hfill\small{4 Jan 2024}
\vspace{0.5em}
\hrule
\vspace{-0.5em}
\section{Contraction Mapping and Comparison Lemma}

\subsection{Math Review}

\subsubsection{Vector Spaces}

The set of all n dimensional vector \(x = [x_1, x_2, ..., x_n]^{\top} \) with each element being 
a real number is denoted as \(\mathbb{R}^n\) and defines the \(n\)-dimensional Euclidean space.
The inner product of two vectors \(x\) and \(y\) is defined as:
\[
    x^{\top} y = \sum\limits_{i=1}^{n} x_i y_i
\]

The norm of a vector \(x\) is a real valued function with the properties:

\begin{itemize}
    \item \(\lVert x \rVert \geq 0 \quad \forall x \in \mathbb{R}^n\) with \(\lVert x \rVert = 0 \iff x = 0\)
    \item \(\lVert \alpha x \rVert = \lvert \alpha \rvert \lVert x \rVert \quad \forall \alpha \in \mathbb{R}\)
    \item \(\lVert x + y \rVert \leq \lVert x \rVert + \lVert y \rVert \quad \forall x, y \in \mathbb{R}^n\)
\end{itemize}

The class of \(p\)-norm is defined as:
\[
    \lVert x \rVert_p = \left( \sum\limits_{i=1}^{n} \lvert x_i \rvert^p \right)^{\frac{1}{p}} 
    , \quad 1 \leq p < \infty 
\]
with the special case of \(p = \infty\) being:
\[
    \lVert x \rVert_{\infty} = \max\limits_{i} \lvert x_i \rvert
\]

The most commonly used norms are the \(1\)-norm, \(2\)-norm and \(\infty\)-norm are defined as:
\[
    \begin{aligned}
        \lVert x \rVert_1 &= \sum\limits_{i=1}^{n} \lvert x_i \rvert \\
        \lVert x \rVert_2 &= \sqrt{\sum\limits_{i=1}^{n} \lvert x_i \rvert^2} \\
        \lVert x \rVert_{\infty} &= \max\limits_{i} \lvert x_i \rvert
    \end{aligned}
\]

All \(p\)-norms are equivalent in the sense that if \(\lVert \cdot \rVert_\alpha\) and
\(\lVert \cdot \rVert_\beta\) are two \(p\)-norms, then there exists two positive constants
\(c_1\) and \(c_2\) such that:
\[
    c_1 \lVert x \rVert_\alpha \leq \lVert x \rVert_\beta \leq c_2 \lVert x \rVert_\alpha \quad \forall x \in \mathbb{R}^n
\]
An important property of the \(p\)-norm is the Holder's inequality: 
\[
    \lvert x^{\top} y \rvert \leq \lVert x \rVert_p \lVert y \rVert_q \quad,
    \frac{1}{p} + \frac{1}{q} = 1, \quad  \forall x, y \in \mathbb{R}^n
\]
Many times when the properties deduced from the basic properties satisfied by any norm, in 
such cases the norm is denoted by \(\lVert \cdot \rVert\) without any subscript, indicating
that the norm can be any \(p\)-norm.

The matrix A defines a linear mapping \(y = Ax\) from \(\mathbb{R}^n\) to \(\mathbb{R}^m\).
The induced norm of the matrix A is defined as:
\[
    \lVert A \rVert_p = \sup\limits_{x \neq 0} \frac{\lVert Ax \rVert_p}{\lVert x \rVert_p}
    = \max\limits_{\lVert x \rVert_p = 1} \lVert Ax \rVert_p \quad 1 \leq p \leq \infty
\]
This norm for p = 1, 2 and \(\infty\) is defined as:
\[
    \begin{aligned}
        \lVert A \rVert_1 &= \max\limits_{1 \leq j \leq n} \sum\limits_{i=1}^{m} \lvert a_{ij} \rvert \\
        \lVert A \rVert_2 &= \sqrt{\lambda_{\max}(A^{\top} A)} \\
        \lVert A \rVert_{\infty} &= \max\limits_{1 \leq i \leq m} \sum\limits_{j=1}^{n} \lvert a_{ij} \rvert
    \end{aligned}  
\]
where \(\lambda_{\max}(A^{\top} A)\) is the largest eigenvalue of the matrix \(A^{\top} A\).

Some useful properties of the induced matrix norm are for real matrices \(A \in \mathbb{R}^{m \times n}\)
and \(B \in \mathbb{R}^{n \times l}\) are:
\[
    \begin{aligned}
        \frac{1}{\sqrt{n}} \lVert A \rVert_\infty &\leq \lVert A \rVert_2 \leq \sqrt{m} \lVert A \rVert_\infty \\
        \frac{1}{\sqrt{m}} \lVert A \rVert_1 &\leq \lVert A \rVert_2 \leq \sqrt{n} \lVert A \rVert_1 \\
        \lVert A \rVert_2 & \leq \sqrt{\lVert A \rVert_1 \lVert A \rVert_\infty} \\
        \lVert AB \rVert_p & \leq \lVert A \rVert_p \lVert B \rVert_p \\
    \end{aligned}
\]

\subsubsection{Sequence, Series and Sets} 

\textbf{Convergence of Sequence}: A sequence of vectors \(x_0,x_1, \dots , x_k, \dots\) 
in \(\mathbb{R}^n\) denoted by \(\{x_k\}\), is said to converge to a a limit vector \(x\) if :
\[
    \lVert x_k - x \rVert \to 0 \quad \text{as} \quad k \to \infty
\]
which is equivalent to:
\[
    \lVert x_k - x \rVert < \epsilon \quad \forall k \geq N \text{ for some } 
    N \in \mathbb{N} \text{ and } \epsilon > 0
\] 
A vector \(x\) is an accumulation point of the sequence \(\{x_k\}\) if there exists a subsequence
\(\{x_{k_j}\}\) of \(\{x_k\}\) that converges to \(x\).

A bounded sequence \(\{x_k\}\) in \(\mathbb{R}^n\) has atleast one accumulation point in \(\mathbb{R}^n\).
A sequence \(r_k\) is said to be increasing if \(r_k \leq r_{k+1}\) for all \(k \in \mathbb{N}\).
It is said to be strictly increasing if \(r_k < r_{k+1}\) for all \(k \in \mathbb{N}\). And similarly
for decreasing and strictly decreasing sequences.

An increasing sequence bounded from above converges to a real number. Similarly, a decreasing sequence
bounded from below converges to a real number.


\textbf{Sets}: A subset \(S \subset \mathbb{R}^n\) is said to be open if for every \(x \in S\), there exists
a neighborhood of \(x\):
\[
    \mathcal{N}_\epsilon(x) = \{z \in \mathbb{R}^n \mid \lVert z - x \rVert < \epsilon\}  
\] 
such that \(\mathcal{N}_\epsilon(x) \subset S\). A set is said to be closed if its complement is open.
Equivalently, a set is closed if and only if every convergent sequence in the set \(S\)
has its limit in \(S\).

A set is said to be bounded if there is \(r>0\) such that
\[
    \lVert x \rVert < r \quad \forall x \in S
\]
Thus, a compact set is a set that is closed and bounded.

A point \(p\) is a boundary point of a set \(S\) if every neighborhood of \(p\) contains at least one
point in \(S\) and at least one point not in \(S\). The set of all boundary points of \(S\) is called
the boundary of \(S\) and is denoted by \(\partial S\). A closed set contains all its boundary points,
with open sets containing none of its boundary points. The interior of a set \(S\) is the set of all
points in \(S\) that are not boundary points of \(S\) i.e. \(S \setminus \partial S\).
The closure of a set \(S\) is the union of \(S\) and its boundary i.e. \(\overline{S} \coloneqq S \cup \partial S\). 

A open set \(S\) is connected if every pair of points in \(S\) can be joined by a curve in \(S\). The
set \(S\) is called convex if for every pair of points \(x, y \in S\), and for every \(\lambda \in [0,1]\),
\(\lambda x + (1 - \lambda) y \in S\).

\subsubsection{Normed Linear Spaces}
A linear space \(\mathcal{X} \) is a normed linear space if, to each vector \(x \in \mathcal{X}\),
there is a real valued norm \(\lVert x \rVert \) that satifies the following properties:
\begin{itemize}
    \item \(\lVert x \rVert \geq 0 \quad \forall x \in \mathcal{X}\) with \(\lVert x \rVert = 0 \iff x = 0\)
    \item \(\lVert \alpha x \rVert = \lvert \alpha \rvert \lVert x \rVert \quad \forall \alpha \in \mathbb{R}\)
    \item \(\lVert x + y \rVert \leq \lVert x \rVert + \lVert y \rVert \quad \forall x, y \in \mathcal{X}\)
\end{itemize}
To denote that a norm \(\lVert \cdot \rVert \) is a norm on the linear space \(\mathcal{X}\), we write
the norm as \(\lVert \cdot \rVert_\mathcal{X} \).

\textbf{Convergence} A sequence \(\{x_k\} \in \mathcal{X} \) is said to converge to \(x \in \mathcal{X}\)
if:
\[
    \lVert x_k - x \rVert \to 0 \quad \text{as} \quad k \to \infty
\]
\textbf{Closed Set}: A set \(S \subset \mathcal{X}\) is said to be closed if every convergent sequence
in \(S\) has its limit in \(S\).

\textbf{Cauchy Sequence}: A sequence \(\{x_k\} \in \mathcal{X}\) is said to be a Cauchy sequence if:
\[
    \lVert x_k - x_l \rVert \to 0 \quad \text{as} \quad k, l \to \infty
\]
Every convergent sequence is a Cauchy sequence, the converse is true if the space \(\mathcal{X}\) is
complete.

\textbf{Banch Space}: A normed linear space \(\mathcal{X}\) is said to be a complete space or a Banch space
if every Cauchy sequence in \(\mathcal{X}\) converges to a vector in \(\mathcal{X}\).

\subsection{Contraction Mapping}
The motivation for using contraction mapping in the non linear systems is that the solution of 
the non linear system can be said as a \emph{fixed point} of the non linear map.

Consider the system \(\dot{x} = f(t,x(t)) \; x(t_0) = x_0 \; t \in [t_0, t_f]\). The solution of the
the system can be written as:
\[
    x(t) = x_0 + \int\limits_{t_0}^{t} f(s, x(\tau )) d\tau
\]
Envisioning this as solving the system in one go we can the above as:
\[
    x(\cdot) = x_0 + \int\limits_{t_0}^{(\cdot)} f(s, x(\tau )) d\tau
\]
In other words, the solution of the system is a fixed point of the map \(F\): 
\[
    x(\cdot) = F(x(\cdot)) \quad \text{ where }F \text{ is some map}  
\]
This can be explained using a linear system. Consider the system \(\dot{x} = Ax, \; x(t_0) = x_0\).
Then the map \(F\) is defined as:
\[
    F(x(\cdot))(t) = x_0 + \int\limits_{t_0}^{t} A x(\tau) d\tau
\]
A common method of finding the solution of finding such a fixed point is using the successive
approximation method. Thus, the iterations are defined as:
\[
    x_{k+1}(t) = x_0 + \int\limits_{t_0}^{t} A x_k(\tau) d\tau
\]
Thus, we have the following
\[
    \begin{aligned}
        x_0(s) &= x_0 \quad \forall s \in [t_0, t_1] \\
        x_1(t) &= x_0 + \int\limits_{t_0}^{t} A x_0(\tau) d\tau = \left[ I + A(t-t_0) \right] x_0\\
        x_2(t) &= \cdots  = \left[ I + A(t-t_0) + \frac{A^2(t-t_0)^2}{2!} \right] x_0 \\
        \implies x_k(t) &= e^{A(t-t_0)} x_0
    \end{aligned}
\]

Thus, we can now define the contraction theorem as follows:
\begin{theorem}[Contraction Theorem]
    Let \(\mathcal{X} \) be a complete normed linear space and let \(\mathcal{S} \) be a closed subset 
    of \(\mathcal{X} \). Let \(T : \mathcal{S} \to  \mathcal{S} \) such that:
    \[
        \lVert T(x) - T(y) \rVert \leq \rho  \lVert x - y \rVert \quad \forall x, y \in \mathcal{S}, \;
        \rho \in [0,1)
    \]
    Then,
    \begin{itemize}
        \item there exits  unique vector \(x^{\star} \in \mathcal{S}\) such that \(T(x^{\star}) = x^{\star}\)
        \item \(x^{\star} \) can be obtained by fixed point iteration i.e. 
        \[
            x^{\star}  = \lim\limits_{k \to \infty} x_k \quad \text{where}, \; x_{k+1} = T(x_k), \;
            x_0 \in \mathcal{S}
        \]
    \end{itemize}
\end{theorem}
\begin{proof}
    Choose an arbitrary \(x_0 \in \mathcal{S}\) and define,
    \[
        x_{k+1} = T(x_k) \quad T : \mathcal{S} \to \mathcal{S} \implies x_k \in \mathcal{S} \quad
         \forall k \geq 0
    \]
    The proof follows the following series of claims:
    \begin{claims}
        \(\{x_k\}\) is a Cauchy sequence.
    \end{claims}
    \[
        \begin{aligned}
            \lVert x_{k+1} - x_k \rVert = \lVert T(x_k) - T(x_{k-1}) \rVert &\leq \rho \lVert x_k - x_{k-1} \rVert\\
            &\leq \rho^2 \lVert x_{k-1} - x_{k-2} \rVert \\
            &\leq \rho^k \lVert x_1 - x_0 \rVert
        \end{aligned}
    \]
    Now, 
    \[
        \begin{aligned}
            \lVert x_{k+r} - x_k \rVert &\leq \lVert x_{k+r} - x_{k+r-1} \rVert +
             \lVert x_{k+r-1} - x_{k+r-2} \rVert + \cdots + \lVert x_{k+1} - x_k \rVert \\
             & \leq \left( 
                    \rho^{k+r-1} + \rho^{k+r-2} + \cdots + \rho^k
              \right) 
                \lVert x_1 - x_0 \rVert \\
                & \leq \frac{\rho^k}{1 - \rho} \lVert x_1 - x_0 \rVert \implies \{x_k\} \text{ is a Cauchy sequence}
        \end{aligned}
    \]
    \begin{claims}
        \(\{x_k\}\) converges to some \(x^{\star} \in \mathcal{S}\) as \(k \to \infty\).
    \end{claims}
    Since \(\mathcal{S}\) is complete and closed, \(\{x_k\}\) converges to some \(x^{\star} \in \mathcal{S}\).
    \begin{claims}
        \(x^{\star} \) is a fixed point of \(T\). i.e. \(T(x^{\star}) = x^{\star}\)
    \end{claims}
    We have:
    \[
        \begin{aligned}
            \lVert x^{\star} - T(x^{\star}) \rVert &\leq  \lVert x^{\star} - x_k \rVert + \lVert x_k - T(x^{\star}) \rVert \\
            & = \lVert x^{\star} - x_k \rVert + \lVert T(x_k) - T(x^{\star}) \rVert \\
            &\leq \underbrace{ \lVert x^{\star} - x_k \rVert + \rho \lVert x_k - x^{\star} \rVert}_{\to 0 \text{ as } k \to \infty} \\
            & \implies \lVert x^{\star} - T(x^{\star}) \rVert = 0 \implies x^{\star} = T(x^{\star})
        \end{aligned}
    \]

    \begin{claims}
        \(x^{\star} \) is unique fixed point of \(T \in \mathcal{S}\).
    \end{claims}
    \textbf{By Contradiction}: Let \(x^{\star}\) and \(y^{\star}\) be two fixed points of \(T\) such that 
    \(y^{\star} = T(y^{\star})\), \(x^{\star} = T(x^{\star})\) and \(x^{\star} \neq y^{\star}\). Then,
    \[
        \begin{aligned}
            \lVert x^{\star} - y^{\star} \rVert &= \lVert T(x^{\star}) - T(y^{\star}) \rVert \\
            &\leq \rho \lVert x^{\star} - y^{\star} \rVert \implies \lVert x^{\star} - y^{\star} \rVert = 0 \\
            &\implies x^{\star} = y^{\star} \quad \text{which is a contradiction}
        \end{aligned}
    \]  
    Thus, \(x^{\star}\) is the unique fixed point of \(T\).
\end{proof}\vspace{1em}
We can now use this result to prove the local existence and uniqueness of the solution of the the 
nonlinear system \(\dot{x} = f(t,x(t)), \; x(t_0) = x_0, \; t \in [t_0, t_f]\), where \(f\) is piecewise
continuous.
\begin{theorem}[Local Existence and Uniqueness]
    Let \(\dot{x} = f(t,x(t)), \; x(t_0) = x_0, \; t \in [t_0, t_f]\) be a nonlinear system where \(f\) is
    piecewise continuous, and satifies the lipschitz condition,
    \[
        \lVert f(t,x) - f(t,y) \rVert \leq L \lVert x - y \rVert, \;
        t \in [t_0, t_f], \; L > 0  \quad \forall x, y \in B_r(x_0) \coloneqq \{x \mid \lVert x - x_0 \rVert \leq r\}
    \]
    Then, \( \exists \; \delta > 0\) such that the system has a unique solution on the interval
    \([t_0, t_0 + \delta]\).
\end{theorem}
\begin{proof}
    Define the map \(P\) as:
    \[
        P(\cdot)(t) \coloneqq  x_0 + \int\limits_{t_0}^{t} f(s, x(\tau)) d\tau \implies x(t) = P(x(\cdot))(t)
    \]
    Let,
    \[
        \mathcal{X}  \coloneqq  C[t_0, t_0 + \delta] \to \text{ set of continuous functions on } [t_0, t_0 + \delta] 
    \]
    Note on abuse of notation. 
    \[
        x \in \mathcal{X} \quad \text{ but } \quad x(t) \in \mathbb{R}^n
    \]
    Defining the norm on \(\mathcal{X}\) as:
    \[
        {\lVert x \rVert}_C \coloneqq \max\limits_{t \in [t_0, t_0 + \delta]} \lVert x(t) \rVert  
    \]
    The above norm is genrelising \(\infty \)-norm to function spaces.
    Thus, we have:
    \[
        \mathcal{S} \coloneqq \{x \in \mathcal{X} \mid {\lVert x - x_0 \rVert}_C \leq r\}
    \]
    Thus, we have \(\mathcal{S} : \mathcal{X} \to \mathcal{X}\). But we need to show that 
    \( P : \mathcal{S} \to \mathcal{S}\). To achieve this we note the following two observations:
    \begin{observe}[1]
        \(f\) is piecewise continuous on \([t_0, t_0 + \delta]\) and thus \( \lVert f(t,x) \rVert\) 
        is piecewise continuous too.
        Let,
        \[
            h \coloneqq \max\limits_{t \in [t_0, t_0 + \delta]} \lVert f(t,x) \rVert
        \]
    \end{observe}
    \begin{observe}[2]
        \[
            \begin{aligned}
                \lVert P(x(\cdot))(t) - x_0 \rVert &= \left\lVert \int\limits_{t_0}^{t} f(\tau, x(\tau)) d\tau \right\rVert \\
                &\leq \int\limits_{t_0}^{t} \left\lVert f(\tau , x(\tau)) - f(\tau, x_0) + f(\tau, x_0) \right\rVert d\tau\\
                &\leq \int\limits_{t_0}^{t} \lVert f(\tau, x(\tau)) - f(\tau, x_0) \rVert +
                 \lVert f(\tau, x_0) \rVert d\tau \\
                &\leq \int\limits_{t_0}^{t} L \lVert x(\tau) - x_0 \rVert + h d\tau 
                \leq \int\limits_{t_0}^{t} L r + h d\tau = (t - t_0) (Lr + h) \\&= \delta (Lr + h)
            \end{aligned}
        \]
        Since this is true for all \(t \in [t_0, t_0 + \delta]\), we have:
        \[
            \lVert P(x(\cdot)) - x_0 \rVert \leq \delta (Lr + h) \leq \underbrace{r}_{\text{enforce this}}
        \]
        The condition on \(\delta \) is:
        \[
            \delta (Lr + h) \leq r \implies \delta \leq \frac{r}{Lr + h}  
        \]
    \end{observe}
    Now, choose \(\delta \) such that \(P: \mathcal{S} \to \mathcal{S} \) is a contraction mapping. Thus,
    \[
        \begin{aligned}
            \lVert P(x(\cdot))(t) - P(y(\cdot))(t) \rVert &= \left\lVert \int\limits_{t_0}^{t} f(\tau, x(\tau)) -
             f(\tau, y(\tau)) d\tau \right\rVert \\
                &\leq \int\limits_{t_0}^{t} \lVert f(\tau, x(\tau)) - f(\tau, y(\tau)) \rVert d\tau \\
                &\leq \int\limits_{t_0}^{t} L \lVert x(\tau) - y(\tau) \rVert d\tau \\
                &\leq L \delta \lVert x - y \rVert_C \quad \forall \; t \in [t_0, t_0 + \delta] \\
                &\leq \rho \lVert x(\cdot) - y(\cdot) \rVert_C \quad \rho \in [0,1)\\
                & \implies  \delta \leq \frac{\rho}{L}
        \end{aligned}
    \]
    Picking \(\delta \leq \min\left\{ \frac{r}{Lr + h}, \frac{\rho}{L}, t_1 - t_0 \right\}\) we have the contraction
    on the \(P\). Thus, \(P(x(\cdot))\) has a unique fixed point \(x^{\star} \in \mathcal{S}\). Now,
    we need to extend the result from map \(\mathcal{S}\) to map \(\mathcal{X}\).

    Even if there are multiple solutions in \(\mathcal{X}\), it must be that \(x(t) \in B_r(x_0) \forall 
    t \in [t_0, t_0 + \delta]\) for some \(\mu > 0 \).
    Let, \(t_0 + \mu \) be the first time when \(\lVert x(t) - x_0 \rVert < r \). Then, we have:
    \[
    \begin{aligned}
        r &= \left\lVert \int\limits_{t_0}^{t_0 + \mu} f(\tau, x(\tau)) d\tau \right\rVert \\
        &\leq \int\limits_{t_0}^{t_0 + \mu} Lr + h d\tau = \mu (Lr + h) \\
        &\implies \mu \geq \frac{r}{Lr + h} \geq \delta
    \end{aligned}    
    \]
    Thus, the same result holds for \(\mathcal{X}\) too, completing the proof.
\end{proof}
\subsection{Lipschitz Condition}
A function \(f : \mathbb{R}^n \to \mathbb{R}^n\) is said to satisfy the Lipschitz condition in \(x\) at \(x_0\)
if \(\exists L > 0, r > 0\) such that:
\[
    \lVert f(t,x) - f(t,y) \rVert \leq L \lVert x - y \rVert \quad \forall
     x, y \in B_r(x_0) \; \forall t \in [t_0, t_f]
\]
\begin{definition}[Locally Lipschitz]
    A function \(f : \mathbb{R}^n \to \mathbb{R}^n\) is said to be \emph{locally lipschitz} on a 
    set \(D \subset \mathbb{R}^n\) if \(\forall x_0 \in D\), the function \(f\) satisfies the
    lipschitz condition for lipschitz constant \(L(x_0)\).\\
    The function is said to be lipschitz on \(D\) if the function satifies the lipschitz condition
    for all \(x \in D\) with a uniform lipschitz constant \(L\).
\end{definition}
The function \(f : \mathbb{R} ^n \to \mathbb{R} ^n\) is said to be globally lipschitz if the function
is lipschitz on the entire domain \(\mathbb{R}^n\).
\begin{example}[Lipschitz Condition]
    Let \(f: I \subset \mathbb{R} \to \mathbb{R} \ni \vert f^{\prime} (x) \vert \leq k, \forall x \in I\).
    Then, \(f\) satisfies the Lipschitz condition on I with \(L = k\):
    \[
        \lvert f(x) - f(y) \rvert = \left\lvert \int\limits_{x}^{y} f^{\prime} (s) ds \right\rvert \leq
        \int\limits_{x}^{y} \lvert f^{\prime} (s) \rvert ds \leq k \lvert x - y \rvert  
    \]
\end{example}
\begin{lemma}[Lipschitz Condition for a Convex Set]
    Let \(f : [a,b] \times D \to \mathbb{R}^n\) be a continuous on some domain \(D \subset \mathbb{R}^n\). Suppose
    \(\frac{\partial f}{\partial x} \) exists and is continuous on \([a,b] \times D\). If for a convex subset
    \(W \subset D, \; \exists L > 0\) such that:
    \[
        \left\lVert \frac{\partial f}{\partial x} (t,x) \right\rVert \leq L \quad \forall (t,x) \in [a,b] \times W
    \]
    Then,
    \[
        \lVert f(t,x) - f(t,y) \rVert \leq L \lVert x - y \rVert \quad \forall \; (t,x), (t,y) \in [a,b] \times W
    \]
\end{lemma}
\begin{lemmaproof}
    For a convex set, we have:
    \[
        \begin{aligned}
            \gamma (s) &= x_1(1-s) + x_2 s \quad s \in [0,1] \\
            g(s) &\coloneqq f(t, \gamma(s)) \\
        \end{aligned}
        \]
        \[
        \begin{aligned}
            \implies \lVert f(t,x_1) - f(t,x_2) \rVert &= \lVert g(0) - g(1) \rVert  = 
            \left\lVert \int\limits_{0}^{1} \frac{\partial g}{\partial s} \mathrm{d}s \right\rVert \\  
            &\leq \int\limits_{0}^{1} \left\lVert \frac{\partial g}{\partial \gamma } 
            \frac{\partial \gamma}{\partial s} \right\rVert \mathrm{d}s \\
            &\leq \int\limits_{0}^{1} L \lVert x_2 - x_1 \rVert \mathrm{d}s = L \lVert x_2 - x_1 \rVert
        \end{aligned}
    \]
    Thus, \(f\) satisfies the Lipschitz condition on \(W\).
\end{lemmaproof}
\begin{lemma}
    If \(f(t,x)\) and \(\frac{\partial f}{\partial x} (t,x)\) are continuous on \([a,b] \times D\), or some
    domain \(D \subset \mathbb{R}^n\), then \(f\) is locally lipschitz in \(x\) on \([a,b] \times D\). 
\end{lemma}
\begin{lemma}
    Supoose \(f(t,x)\) and \(\frac{\partial f}{\partial x} (t,x)\) are continuous on \([a,b] \times \mathbb{R} ^n\).
    Then, \(f\) is globally lipschitz in \(x\) on \([a,b] \times \mathbb{R} ^n\) iff 
    \(\frac{\partial f}{\partial x} (t,x)\) is uniformly bounded on \([a,b] \times \mathbb{R} ^n\).
\end{lemma}

\begin{example}
    \[
        \begin{aligned}
            f(x) &\coloneqq \begin{bmatrix}
                -ax_1 + x_1 x_{2}  \\
                ax_2 - x_1 x_2 \\
            \end{bmatrix} \quad a > 0\\
            \frac{\partial f}{\partial x} (x) &\coloneqq \begin{bmatrix}
                -a + x_2 & x_1 \\
                -x_2 & a - x_1 \\
            \end{bmatrix}
        \end{aligned}
    \]
    Since \(f(x)\) is differentiable and continuous over \(\mathbb{R}^2\), \(f(x)\) is locally lipschitz. 
    But since \(\frac{\partial f}{\partial x} (x)\) is not uniformly bounded, \(f(x)\) is not globally lipschitz.
    
    But, it can be lipschitz on some compact set. Let,
    \[
        W \coloneqq \{x \in \mathbb{R}^2 \mid \vert x_1 \vert < a_1, \vert x_2 \vert < a_2\}
    \]
    Here, we can explot the equivalence of \(p\)-norms to chose and find the lipschitz constant. Choose 
    induced \(\infty \) norm:
    \[
        \left\lVert \frac{\partial f}{\partial x} (x) \right\rVert_{\infty} = \max\left\{ \lvert -a + x_2 \rvert, \lvert x_1 \rvert,
        \lvert -x_2 \rvert, \lvert a - x_1 \rvert \right\} \leq a + a_1 + a_2 \eqqcolon L
    \] 
\end{example}
\subsection{Existence and Uniqueness of Solutions}
\begin{theorem}
    Let \(f(t,x)\) be a piecewise continuous function in \(t\), and locally lipschitz in \(x, \; \forall t \geq t_0\),
    and \(\forall x \in D \subset \mathbb{R}^n\). Let, \(W\) is a compact subset of \(D\) and \(x_0 \in W\). Suppose
    that every solution of \(\dot{x} = f(t,x),\; x(t_0) = x_0\) lies entirly in \(W,\; \forall t \geq  t_0\). Then,
    there exists a unique solution with \(x(t_0) = x_0,\; \forall t \geq t_0 \)   
\end{theorem}
\begin{proof}
    TODO: misunderstood and wrote the wrong proof.
\end{proof}
\begin{lemma}[Comparison Lemma]
    Consider the scalar ODE: \(\dot{u} = f(t,u), u(t_0) = u_0\). Supoose \(ft(t,u)\) is continuous in \(t\) and
    locally lipschitz in \(u, \; \forall t \geq t_0, \text{ and, } u(t) \in J \subset \mathbb{R}, \; \forall t \geq t_0\).
    Let \([t_0,T]\) be the maximal interval of existence of the solution \(u(t) \in J,\; \forall t \in [T_o,T)  \).
    Let \(v(t)\) be continuous such that:
    \[
        D^+ v(t) \leq f(t,v(t)) \quad v(t_0) \leq u_0, with \; v(t) \in J, \; \forall t \in [t_0, T)
    \]
    Then, \(v(t) \leq u(t), \; \forall t \in [t_0, T)\).
\end{lemma}
\begin{lemmaproof}
    Assume that \(V(\cdot)\) is differentiable. The proof holds otherwise too, but assumption greatly simplifies
    the proof.
    Proof by contraction:
    Suppose \(\exists a,b \in [t_0,t_1]\) such that:
    \[
        v(a) = u(a) \quad v(t) < u(t) \quad \forall t \in (a,b]
    \] 
    \[
        \begin{aligned}
            \implies v(t) - v(a) &> u(t) - u(a) \quad \forall t \in (a,b]\\
            \lim_{h \to 0^+} \frac{v(a+h) - v(a)}{h} &\geq \lim_{h \to 0^+} \frac{u(a+h) - u(a)}{h} \\
            \implies \dot{v}(a) &\geq \dot{u}(a) \quad \text{which is a contradiction}            
        \end{aligned}
    \]
\end{lemmaproof}
\begin{theorem}
    Let f(t,x) be piecewise continuous in t, and locally lipschitz in x on \([t_0,t_1] \times W\), with
    Lipschitz constant \(L\), and \(W \subset \mathbb{R}^n\) is a open connected set. Let, \(y(t),z(t)\) be the solutions
    of \(\dot{y} = f(t,y),\; y(t_0) = y_0, \; \dot{z} = f(t,z) + g(t,z),\; z(t_0 ) = z_0\), such that 
    \(y(t),z(t) \in W,\; \forall t \in [t_0,t_1]\). Suppose \(\lVert g(t,x)\rVert \leq  \mu \;\forall (t,x) \in
    [t_0,t_1]\times W,\;\mu >0,\; \lVert y_0 - z_0 \rVert \leq \gamma  \), Then,
    \[
        \lVert y(t) - z(t) \rVert \leq \gamma e^{L(t-t_0)} + \frac{\mu}{L} \left( e^ {L(t-t_0)} - 1\right) 
        \quad \forall t \in [t_0,t_1]  
    \]
\end{theorem}
\begin{proof}
    Note that, if we know more about, the system, then the upper bound can be made tighter and hence, the above upper 
    bound can be useless at times.
    \[
        \begin{aligned}
            y(t) = &y_0 + \int\limits_{t_0}^{t} f(s,y(s)) ds \\
            z(t) = &z_0 + \int\limits_{t_0}^{t} f(s,z(s)) + g(s,z(s)) ds\\
            \implies \lVert y(t) - z(t) \rVert &\leq \lVert y_0 - z_0 \rVert + 
            \int\limits_{t_0}^{t} \lVert f(s,y(s)) - f(s,z(s)) \rVert ds + \int\limits_{t_0}^{t} \lVert g(s,z(s)) \rVert ds\\
            & \leq \gamma  + \mu (t-t_0) + \int\limits_{t_0}^{t} L \lVert y(s) - z(s) \rVert ds
        \end{aligned}
    \]
    Using, the resutls of gronwell-bellman inequality, we have:
    \[
        \lVert y(t) - z(t) \rVert \leq \gamma e^{L(t-t_0)} + \frac{\mu}{L} \left( e^ {L(t-t_0)} - 1\right) 
        \quad \forall t \in [t_0,t_1]  
    \]
    Thus, completing the proof.
\end{proof}